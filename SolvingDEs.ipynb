{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Bounds:\n",
      "T_max: 100000.0\n",
      "I_max: 34122.40023405826\n",
      "L_max: 66.74642335613261\n",
      "V_max: 5929376.922928926\n",
      "\n",
      "Scaled Upper Bounds (with 10% buffer):\n",
      "ub_T: 110000.00000000001\n",
      "ub_I: 37534.64025746409\n",
      "ub_L: 73.42106569174588\n",
      "ub_V: 6522314.615221819\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import deepxde as dde\n",
    "from deepxde.backend import tf\n",
    "from scipy.integrate import solve_ivp, odeint\n",
    "import math\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from scipy.interpolate import interp1d\n",
    "import torch\n",
    "\n",
    "# Initial Conditions\n",
    "T0 = 100000\n",
    "I0 = 0\n",
    "L0 = 0\n",
    "V0 = 200\n",
    "\n",
    "# Parameters\n",
    "d = 0.01\n",
    "beta = 0.0000002\n",
    "f = 0.001\n",
    "a = 0.2\n",
    "delta_I = 1 \n",
    "delta_L = 0.0039\n",
    "c = 23\n",
    "p = 4000\n",
    "lambda_param = 1000  # T0 * d\n",
    "Omega = 1\n",
    "\n",
    "t_initial = 0\n",
    "t_final = 100\n",
    "\n",
    "t = np.linspace(t_initial, t_final, 1000)\n",
    "\n",
    "\n",
    "\n",
    "geom = dde.geometry.TimeDomain(t_initial, t_final)\n",
    "\n",
    "Y0 = [T0, I0, L0, V0]\n",
    "\n",
    "def system(Y, t):\n",
    "    T, I, L, V = Y\n",
    "    dT_dt = lambda_param - d * T - Omega * beta * V * T\n",
    "    dI_dt = (1 - f) * Omega * beta * V * T + a * L - delta_I *   I\n",
    "    dL_dt = f * Omega * beta * V * T - a * L - delta_L * L\n",
    "    dV_dt = Omega * p * I - c * V\n",
    "    return [dT_dt, dI_dt, dL_dt, dV_dt]\n",
    "\n",
    "t = np.linspace(t_initial, t_final, 1000)\n",
    "\n",
    "solution = odeint(system, Y0, t)\n",
    "T, I, L, V = solution.T\n",
    "\n",
    "\n",
    "T_max = np.max(T)\n",
    "I_max = np.max(I)\n",
    "L_max = np.max(L)\n",
    "V_max = np.max(V)\n",
    "\n",
    "print(f\"Upper Bounds:\")\n",
    "print(f\"T_max: {T_max}\")\n",
    "print(f\"I_max: {I_max}\")\n",
    "print(f\"L_max: {L_max}\")\n",
    "print(f\"V_max: {V_max}\")\n",
    "\n",
    "# Set the upper bounds with a 10% buffer\n",
    "ub_T = T_max * 1.1\n",
    "ub_I = I_max * 1.1 if I_max != 0 else 1.0  # Avoid division by zero\n",
    "ub_L = L_max * 1.1 if L_max != 0 else 1.0\n",
    "ub_V = V_max * 1.1\n",
    "\n",
    "print(f\"\\nScaled Upper Bounds (with 10% buffer):\")\n",
    "print(f\"ub_T: {ub_T}\")\n",
    "print(f\"ub_I: {ub_I}\")\n",
    "print(f\"ub_L: {ub_L}\")\n",
    "print(f\"ub_V: {ub_V}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_T = interp1d(t, T, kind='cubic', fill_value='extrapolate')\n",
    "interp_I = interp1d(t, I, kind='cubic', fill_value='extrapolate')\n",
    "interp_L = interp1d(t, L, kind='cubic', fill_value='extrapolate')\n",
    "interp_V = interp1d(t, V, kind='cubic', fill_value='extrapolate')\n",
    "\n",
    "# Corrected Function to Ensure Proper Shape\n",
    "def func(t):\n",
    "    T_val = interp_T(t)\n",
    "    I_val = interp_I(t)\n",
    "    L_val = interp_L(t)\n",
    "    V_val = interp_V(t)\n",
    "    return np.column_stack((T_val, I_val, L_val, V_val))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ODE residual\n",
    "import torch\n",
    "import deepxde as dde\n",
    "# ... (keep other imports)\n",
    "\n",
    "# ... (keep existing code for initial conditions, parameters, and ODE system)\n",
    "\n",
    "# Define ODE residual\n",
    "def ode(t, Y):\n",
    "    T = Y[:, 0:1]\n",
    "    I = Y[:, 1:2]\n",
    "    L = Y[:, 2:3]\n",
    "    V = Y[:, 3:4]\n",
    "    \n",
    "    dT_dt = dde.grad.jacobian(Y, t, i=0)\n",
    "    dI_dt = dde.grad.jacobian(Y, t, i=1)\n",
    "    dL_dt = dde.grad.jacobian(Y, t, i=2)\n",
    "    dV_dt = dde.grad.jacobian(Y, t, i=3)\n",
    "    \n",
    "    return [\n",
    "        dT_dt - (lambda_param / ub_T - d * T - Omega * beta * V * ub_V * T),\n",
    "        dI_dt - ((1 - f) * Omega * beta * V * ub_V * T + a * L * ub_L / ub_I - delta_I * I),\n",
    "        dL_dt - (f * Omega * beta * V * ub_V * T / ub_L - a * L - delta_L * L),\n",
    "        dV_dt - (Omega * p * I * ub_I / ub_V - c * V)\n",
    "    ]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "net = dde.nn.FNN([1] + [64] * 4 + [4], \"tanh\", \"Glorot normal\")\n",
    "\n",
    "# Define the output modification function to enforce hard initial conditions\n",
    "def enforce_initial_conditions(t, y):\n",
    "    t_scaled = (t - t_initial) / (t_final - t_initial)\n",
    "    return t_scaled * y + torch.tensor([T0 / ub_T, I0 / ub_I, L0 / ub_L, V0 / ub_V])\n",
    "\n",
    "# Apply the output modification\n",
    "net.apply_output_transform(enforce_initial_conditions)\n",
    "\n",
    "# ... (rest of the code remains the same)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dde.Model(data, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "'compile' took 0.000369 s\n",
      "\n",
      "Training model...\n",
      "\n",
      "Step      Train loss                                  Test loss                                   Test metric\n",
      "0         [1.00e+00, 1.49e+03, 1.57e-01, 1.92e+01]    [1.00e+00, 1.49e+03, 1.57e-01, 1.92e+01]    []  \n",
      "1000      [1.00e+00, 8.94e-05, 2.70e-04, 2.46e-04]    [1.00e+00, 8.90e-05, 2.70e-04, 2.46e-04]    []  \n",
      "2000      [9.99e-01, 1.57e-05, 4.99e-05, 4.98e-05]    [9.99e-01, 1.56e-05, 4.99e-05, 4.98e-05]    []  \n",
      "3000      [9.98e-01, 2.09e-05, 3.93e-05, 3.04e-05]    [9.98e-01, 2.09e-05, 3.93e-05, 3.04e-05]    []  \n",
      "4000      [9.97e-01, 4.63e-06, 1.38e-05, 8.85e-06]    [9.97e-01, 4.64e-06, 1.38e-05, 8.84e-06]    []  \n",
      "5000      [9.97e-01, 2.01e-06, 7.40e-06, 4.92e-06]    [9.97e-01, 2.01e-06, 7.39e-06, 4.92e-06]    []  \n",
      "6000      [9.96e-01, 3.07e-05, 4.50e-06, 3.05e-06]    [9.96e-01, 3.07e-05, 4.49e-06, 3.05e-06]    []  \n",
      "7000      [9.95e-01, 6.06e-06, 3.52e-06, 2.03e-06]    [9.95e-01, 6.03e-06, 3.52e-06, 2.03e-06]    []  \n",
      "8000      [9.95e-01, 7.86e-01, 2.15e-05, 1.08e-02]    [9.95e-01, 7.86e-01, 2.15e-05, 1.08e-02]    []  \n",
      "9000      [9.94e-01, 7.84e-05, 2.09e-04, 3.13e-04]    [9.94e-01, 7.74e-05, 2.09e-04, 3.14e-04]    []  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m      4\u001b[0m     loss_weights\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m1e4\u001b[39m, \u001b[38;5;241m1e2\u001b[39m, \u001b[38;5;241m1e-5\u001b[39m]  \u001b[38;5;66;03m# Only PDE residuals\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m losshistory, train_state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\utils\\internal.py:22\u001b[0m, in \u001b[0;36mtiming.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     21\u001b[0m     ts \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[1;32m---> 22\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     te \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:657\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, iterations, batch_size, display_every, disregard_previous_best, callbacks, model_restore_path, model_save_path, epochs)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iterations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo iterations for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_name))\n\u001b[1;32m--> 657\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_sgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisplay_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_train_end()\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:675\u001b[0m, in \u001b[0;36mModel._train_sgd\u001b[1;34m(self, iterations, display_every)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_batch_begin()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mset_data_train(\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtrain_next_batch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m    674\u001b[0m )\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_aux_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:567\u001b[0m, in \u001b[0;36mModel._train_step\u001b[1;34m(self, inputs, targets, auxiliary_vars)\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(inputs, targets, auxiliary_vars)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliary_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;66;03m# TODO: auxiliary_vars\u001b[39;00m\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(\n\u001b[0;32m    571\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_state, inputs, targets\n\u001b[0;32m    572\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:364\u001b[0m, in \u001b[0;36mModel._compile_pytorch.<locals>.train_step\u001b[1;34m(inputs, targets, auxiliary_vars)\u001b[0m\n\u001b[0;32m    361\u001b[0m     total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\adam.py:205\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 205\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    208\u001b[0m     params_with_grad: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:361\u001b[0m, in \u001b[0;36mModel._compile_pytorch.<locals>.train_step.<locals>.closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m    359\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(losses)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 361\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    \"adam\",\n",
    "    lr=0.001,\n",
    "    loss_weights=[1e-6, 1e4, 1e2, 1e-5]  # Only PDE residuals\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "losshistory, train_state = model.train(iterations=20000, display_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dde.utils.external.plot_loss_history(losshistory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m t_plot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(t_initial, t_final, \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m PINN_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_plot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m T_pinn \u001b[38;5;241m=\u001b[39m PINN_pred[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m I_pinn \u001b[38;5;241m=\u001b[39m PINN_pred[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:897\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, operator, callbacks)\u001b[0m\n\u001b[0;32m    894\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_predict_begin()\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m operator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 897\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    898\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_predict_end()\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:534\u001b[0m, in \u001b[0;36mModel._outputs\u001b[1;34m(self, training, inputs)\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs, feed_dict\u001b[38;5;241m=\u001b[39mfeed_dict)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaddle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 534\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    536\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mparams, training, inputs)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "t_plot = np.linspace(t_initial, t_final, 1000)\n",
    "\n",
    "PINN_pred = model.predict(t_plot.reshape(-1, 1))\n",
    "\n",
    "T_pinn = PINN_pred[:, 0]\n",
    "I_pinn = PINN_pred[:, 1]\n",
    "L_pinn = PINN_pred[:, 2]\n",
    "V_pinn = PINN_pred[:, 3]\n",
    "\n",
    "# Plot PINN Solution\n",
    "plt.plot(t, np.log10(V_pinn), color=\"orange\", label=\"log10 V (PINN)\")\n",
    "\n",
    "# Plot ODEint Solution\n",
    "plt.plot(t, np.log10(V), color=\"blue\", label=\"log10 V (ODEint)\", linestyle='--')\n",
    "\n",
    "# Adding Labels and Title\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"log10 V\")\n",
    "plt.title(\"Comparison of PINN and ODEint Solutions\")\n",
    "\n",
    "# Adding Legend\n",
    "plt.legend()\n",
    "\n",
    "# Adding Grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the Plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#epsilon is 0 until a certain point. epsilon is 0.9 after a certain point, or estimate\n",
    "#Have data for v for above line\n",
    "\n",
    "#at point m at end of graph haev L(tm)* 10e6 /(L(tm) + I(tm) + T(tm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Bounds:\n",
      "T_max: 100000.0\n",
      "I_max: 34122.40023405826\n",
      "L_max: 66.74642335613261\n",
      "V_max: 5929376.922928926\n",
      "\n",
      "Scaled Upper Bounds (with 10% buffer):\n",
      "ub_T: 110000.00000000001\n",
      "ub_I: 37534.64025746409\n",
      "ub_L: 73.42106569174588\n",
      "ub_V: 6522314.615221819\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# **1. Check and Determine Upper Bounds**\n",
    "# =======================\n",
    "\n",
    "# After solving the ODE system, compute and print the maximum values for each variable\n",
    "T_max = np.max(T)\n",
    "I_max = np.max(I)\n",
    "L_max = np.max(L)\n",
    "V_max = np.max(V)\n",
    "\n",
    "print(f\"Upper Bounds:\")\n",
    "print(f\"T_max: {T_max}\")\n",
    "print(f\"I_max: {I_max}\")\n",
    "print(f\"L_max: {L_max}\")\n",
    "print(f\"V_max: {V_max}\")\n",
    "\n",
    "# Optionally, set the upper bounds based on these maxima\n",
    "ub_T = T_max * 1.1  # Adding 10% buffer\n",
    "ub_I = I_max * 1.1\n",
    "ub_L = L_max * 1.1\n",
    "ub_V = V_max * 1.1\n",
    "\n",
    "print(f\"\\nScaled Upper Bounds (with 10% buffer):\")\n",
    "print(f\"ub_T: {ub_T}\")\n",
    "print(f\"ub_I: {ub_I}\")\n",
    "print(f\"ub_L: {ub_L}\")\n",
    "print(f\"ub_V: {ub_V}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper Bounds:\n",
      "T_max: 100000.0\n",
      "I_max: 34122.40023405826\n",
      "L_max: 66.74642335613261\n",
      "V_max: 5929376.922928926\n",
      "\n",
      "Scaled Upper Bounds (with 10% buffer):\n",
      "ub_T: 110000.00000000001\n",
      "ub_I: 37534.64025746409\n",
      "ub_L: 73.42106569174588\n",
      "ub_V: 6522314.615221819\n",
      "Compiling model...\n",
      "'compile' took 0.000126 s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phase 1: Training with Adam optimizer...\n",
      "Training model...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 205\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# =======================\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# **7. Implementing a Two-Phase Training Strategy**\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# =======================\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Phase 1: Train with Adam optimizer\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Phase 1: Training with Adam optimizer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 205\u001b[0m losshistory, train_state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced iterations for Phase 1\u001b[39;49;00m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[0;32m    208\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Phase 2: Train with L-BFGS optimizer\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Phase 2: Training with L-BFGS optimizer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\utils\\internal.py:22\u001b[0m, in \u001b[0;36mtiming.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     21\u001b[0m     ts \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[1;32m---> 22\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     te \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:643\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, iterations, batch_size, display_every, disregard_previous_best, callbacks, model_restore_path, model_save_path, epochs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mset_data_train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtrain_next_batch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size))\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mset_data_test(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtest())\n\u001b[1;32m--> 643\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_train_begin()\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizers\u001b[38;5;241m.\u001b[39mis_external_optimizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_name):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:832\u001b[0m, in \u001b[0;36mModel._test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_test\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;66;03m# TODO Now only print the training loss in rank 0. The correct way is to print the average training loss of all ranks.\u001b[39;00m\n\u001b[0;32m    829\u001b[0m     (\n\u001b[0;32m    830\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39my_pred_train,\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mloss_train,\n\u001b[1;32m--> 832\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_outputs_losses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_aux_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39my_pred_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mloss_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_losses(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mX_test,\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39my_test,\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mtest_aux_vars,\n\u001b[0;32m    843\u001b[0m     )\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39my_test, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:551\u001b[0m, in \u001b[0;36mModel._outputs_losses\u001b[1;34m(self, training, inputs, targets, auxiliary_vars)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mrequires_grad_(requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 551\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[43moutputs_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliary_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;66;03m# TODO: auxiliary_vars\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:322\u001b[0m, in \u001b[0;36mModel._compile_pytorch.<locals>.outputs_losses_train\u001b[1;34m(inputs, targets, auxiliary_vars)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moutputs_losses_train\u001b[39m(inputs, targets, auxiliary_vars):\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moutputs_losses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliary_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses_train\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:304\u001b[0m, in \u001b[0;36mModel._compile_pytorch.<locals>.outputs_losses\u001b[1;34m(training, inputs, targets, auxiliary_vars, losses_fn)\u001b[0m\n\u001b[0;32m    302\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(inputs)\n\u001b[0;32m    303\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[1;32m--> 304\u001b[0m outputs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# Data losses\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\nn\\pytorch\\fnn.py:47\u001b[0m, in \u001b[0;36mFNN.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinears[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m](x)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[1;32mIn[55], line 118\u001b[0m, in \u001b[0;36moutput_transform\u001b[1;34m(t, y)\u001b[0m\n\u001b[0;32m    115\u001b[0m L \u001b[38;5;241m=\u001b[39m y[:, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m    116\u001b[0m V \u001b[38;5;241m=\u001b[39m y[:, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mhstack([\n\u001b[1;32m--> 118\u001b[0m     T \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_final\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt_initial\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (Y0[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m ub_T),\n\u001b[0;32m    119\u001b[0m     I \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(t \u001b[38;5;241m*\u001b[39m (t_final \u001b[38;5;241m-\u001b[39m t_initial)) \u001b[38;5;241m+\u001b[39m (Y0[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m ub_I),\n\u001b[0;32m    120\u001b[0m     L \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(t \u001b[38;5;241m*\u001b[39m (t_final \u001b[38;5;241m-\u001b[39m t_initial)) \u001b[38;5;241m+\u001b[39m (Y0[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m/\u001b[39m ub_L),\n\u001b[0;32m    121\u001b[0m     V \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(t \u001b[38;5;241m*\u001b[39m (t_final \u001b[38;5;241m-\u001b[39m t_initial)) \u001b[38;5;241m+\u001b[39m (Y0[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m/\u001b[39m ub_V)\n\u001b[0;32m    122\u001b[0m ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:1083\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "# SolvingDEs.ipynb\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import deepxde as dde\n",
    "from deepxde.backend import tf\n",
    "from scipy.integrate import odeint\n",
    "import math\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# =======================\n",
    "# **1. Scaling the Domain and Variables**\n",
    "# =======================\n",
    "\n",
    "# Initial Conditions\n",
    "Y0 = [100000, 0, 0, 200]\n",
    "\n",
    "# Parameters\n",
    "d = 0.01\n",
    "beta = 0.0000002\n",
    "f = 0.001\n",
    "a = 0.2\n",
    "delta_I = 1 \n",
    "delta_L = 0.0039\n",
    "c = 23\n",
    "p = 4000\n",
    "lambda_param = 1000  # T0 * d\n",
    "Omega = 1\n",
    "\n",
    "t_initial = 0\n",
    "t_final = 100\n",
    "\n",
    "t = np.linspace(t_initial, t_final, 1000)\n",
    "\n",
    "# Define ODE system\n",
    "def system(Y, t):\n",
    "    T, I, L, V = Y\n",
    "    dT_dt = lambda_param - d * T - Omega * beta * V * T\n",
    "    dI_dt = (1 - f) * Omega * beta * V * T + a * L - delta_I * I\n",
    "    dL_dt = f * Omega * beta * V * T - a * L - delta_L * L\n",
    "    dV_dt = Omega * p * I - c * V\n",
    "    return [dT_dt, dI_dt, dL_dt, dV_dt]\n",
    "\n",
    "# Solve ODE\n",
    "solution = odeint(system, Y0, t)\n",
    "T, I, L, V = solution.T\n",
    "\n",
    "# =======================\n",
    "# **2. Check and Determine Upper Bounds**\n",
    "# =======================\n",
    "\n",
    "# Compute and print the maximum values for each variable\n",
    "T_max = np.max(T)\n",
    "I_max = np.max(I)\n",
    "L_max = np.max(L)\n",
    "V_max = np.max(V)\n",
    "\n",
    "print(f\"Upper Bounds:\")\n",
    "print(f\"T_max: {T_max}\")\n",
    "print(f\"I_max: {I_max}\")\n",
    "print(f\"L_max: {L_max}\")\n",
    "print(f\"V_max: {V_max}\")\n",
    "\n",
    "# Set the upper bounds with a 10% buffer\n",
    "ub_T = T_max * 1.1\n",
    "ub_I = I_max * 1.1 if I_max != 0 else 1.0  # Avoid division by zero\n",
    "ub_L = L_max * 1.1 if L_max != 0 else 1.0\n",
    "ub_V = V_max * 1.1\n",
    "\n",
    "print(f\"\\nScaled Upper Bounds (with 10% buffer):\")\n",
    "print(f\"ub_T: {ub_T}\")\n",
    "print(f\"ub_I: {ub_I}\")\n",
    "print(f\"ub_L: {ub_L}\")\n",
    "print(f\"ub_V: {ub_V}\")\n",
    "\n",
    "# Create interpolators for the numerical solution\n",
    "interp_T = interp1d(t, T, kind='cubic', fill_value='extrapolate')\n",
    "interp_I = interp1d(t, I, kind='cubic', fill_value='extrapolate')\n",
    "interp_L = interp1d(t, L, kind='cubic', fill_value='extrapolate')\n",
    "interp_V = interp1d(t, V, kind='cubic', fill_value='extrapolate')\n",
    "\n",
    "# Scale solutions\n",
    "def func_scaled(t_scaled):\n",
    "    t_true = t_initial + t_scaled * (t_final - t_initial)\n",
    "    T_val = interp_T(t_true)\n",
    "    I_val = interp_I(t_true)\n",
    "    L_val = interp_L(t_true)\n",
    "    V_val = interp_V(t_true)\n",
    "    return np.column_stack((\n",
    "        T_val / ub_T,\n",
    "        I_val / ub_I,\n",
    "        L_val / ub_L,\n",
    "        V_val / ub_V\n",
    "    ))\n",
    "\n",
    "# =======================\n",
    "# **3. Remove Input Transformations Related to Periodicity**\n",
    "# =======================\n",
    "\n",
    "# Since periodicity is not required, we use only the time variable as input\n",
    "\n",
    "def input_transform(t):\n",
    "    return t  # Identity transform\n",
    "\n",
    "# =======================\n",
    "# **4. Apply Output Transformation to Enforce Initial Conditions**\n",
    "# =======================\n",
    "\n",
    "def output_transform(t, y):\n",
    "    T = y[:, 0:1]\n",
    "    I = y[:, 1:2]\n",
    "    L = y[:, 2:3]\n",
    "    V = y[:, 3:4]\n",
    "    return np.hstack([\n",
    "        T * np.tanh(t * (t_final - t_initial)) + (Y0[0] / ub_T),\n",
    "        I * np.tanh(t * (t_final - t_initial)) + (Y0[1] / ub_I),\n",
    "        L * np.tanh(t * (t_final - t_initial)) + (Y0[2] / ub_L),\n",
    "        V * np.tanh(t * (t_final - t_initial)) + (Y0[3] / ub_V)\n",
    "    ], axis=1)\n",
    "\n",
    "# =======================\n",
    "# **5. Define the PINN Model**\n",
    "# =======================\n",
    "\n",
    "# Define ODE residual\n",
    "def ode(t, Y):\n",
    "    T = Y[:, 0:1]\n",
    "    I = Y[:, 1:2]\n",
    "    L = Y[:, 2:3]\n",
    "    V = Y[:, 3:4]\n",
    "    \n",
    "    dT_dt = dde.grad.jacobian(Y, t, i=0)\n",
    "    dI_dt = dde.grad.jacobian(Y, t, i=1)\n",
    "    dL_dt = dde.grad.jacobian(Y, t, i=2)\n",
    "    dV_dt = dde.grad.jacobian(Y, t, i=3)\n",
    "    \n",
    "    return [\n",
    "        dT_dt - (lambda_param / ub_T - d * T - Omega * beta * V * T),\n",
    "        dI_dt - ((1 - f) * Omega * beta * V * T + a * L - delta_I * I) / ub_I,\n",
    "        dL_dt - (f * Omega * beta * V * T - a * L - delta_L * L) / ub_L,\n",
    "        dV_dt - (Omega * p * I - c * V) / ub_V\n",
    "    ]\n",
    "\n",
    "geom = dde.geometry.TimeDomain(t_initial, t_final)  # Adjusted to actual time range\n",
    "\n",
    "# Define boundary condition (initial conditions)\n",
    "def boundary(t, on_initial):\n",
    "    return on_initial\n",
    "\n",
    "# Define initial conditions as hard constraints\n",
    "ic_T = dde.icbc.IC(geom, lambda t: Y0[0] / ub_T, boundary, component=0)\n",
    "ic_I = dde.icbc.IC(geom, lambda t: Y0[1] / ub_I, boundary, component=1)\n",
    "ic_L = dde.icbc.IC(geom, lambda t: Y0[2] / ub_L, boundary, component=2)\n",
    "ic_V = dde.icbc.IC(geom, lambda t: Y0[3] / ub_V, boundary, component=3)\n",
    "\n",
    "# Define data with balanced loss weights\n",
    "data = dde.data.PDE(\n",
    "    geom,\n",
    "    ode,\n",
    "    [ic_T, ic_I, ic_L, ic_V],\n",
    "    num_domain=5000,\n",
    "    num_boundary=4,  # 4 initial conditions\n",
    "    solution=func_scaled,\n",
    "    num_test=1000\n",
    ")\n",
    "\n",
    "# Define neural network with optimized architecture\n",
    "neurons = 64\n",
    "layers = 4  # Reduced number of hidden layers\n",
    "activation = \"tanh\"\n",
    "initializer = \"Glorot normal\"\n",
    "\n",
    "net = dde.nn.FNN([1] + [neurons] * layers + [4], activation, initializer)\n",
    "\n",
    "# Apply transforms\n",
    "net.apply_feature_transform(input_transform)\n",
    "net.apply_output_transform(output_transform)\n",
    "\n",
    "# Define model\n",
    "model = dde.Model(data, net)\n",
    "\n",
    "# =======================\n",
    "# **6. Adjusting Loss Weights for Multiple Components**\n",
    "# =======================\n",
    "\n",
    "# Balanced loss weights\n",
    "loss_weights = [1.0] * 8  # 4 for PDE residuals and 4 for initial conditions\n",
    "\n",
    "# Update the model compilation with balanced loss weights\n",
    "model.compile(\n",
    "    \"adam\",\n",
    "    lr=0.001,\n",
    "    loss_weights=loss_weights\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# **7. Implementing a Two-Phase Training Strategy**\n",
    "# =======================\n",
    "\n",
    "# Phase 1: Train with Adam optimizer\n",
    "print(\"Starting Phase 1: Training with Adam optimizer...\")\n",
    "losshistory, train_state = model.train(\n",
    "    iterations=10000,  # Reduced iterations for Phase 1\n",
    "    display_every=1000\n",
    ")\n",
    "\n",
    "# Phase 2: Train with L-BFGS optimizer\n",
    "print(\"\\nStarting Phase 2: Training with L-BFGS optimizer...\")\n",
    "model.compile(\"L-BFGS\")\n",
    "losshistory, train_state = model.train()\n",
    "\n",
    "# =======================\n",
    "# **8. Plotting Loss History**\n",
    "# =======================\n",
    "\n",
    "dde.utils.external.plot_loss_history(losshistory)\n",
    "plt.show()\n",
    "\n",
    "# =======================\n",
    "# **9. Making Predictions**\n",
    "# =======================\n",
    "\n",
    "# Predict scaled solutions\n",
    "Y_pinn_scaled = model.predict(t.reshape(-1, 1))\n",
    "\n",
    "# Denormalize predictions\n",
    "Y_pinn = Y_pinn_scaled * np.array([ub_T, ub_I, ub_L, ub_V])\n",
    "\n",
    "# =======================\n",
    "# **10. Comparing with Numerical Solution**\n",
    "# =======================\n",
    "\n",
    "variables = ['T', 'I', 'L', 'V']\n",
    "for i, var in enumerate(variables):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(t, Y_pinn[:, i], label=f\"{var} (PINN)\")\n",
    "    plt.plot(t, solution[:, i], label=f\"{var} (ODEint)\", linestyle='--')\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(var)\n",
    "    plt.title(f\"Comparison of PINN and ODEint Solutions for {var}(t)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# =======================\n",
    "# **End of Script**\n",
    "# ======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Values:\n",
      "T_max: 100000.0\n",
      "I_max: 34125.54548242535\n",
      "L_max: 66.72011290826265\n",
      "V_max: 5928964.0515506575\n"
     ]
    }
   ],
   "source": [
    "# In[0]: Import necessary libraries and set initial parameters\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import deepxde as dde\n",
    "from deepxde.backend import tf\n",
    "from scipy.integrate import odeint\n",
    "import math\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from scipy.interpolate import interp1d\n",
    "import torch\n",
    "\n",
    "# =======================\n",
    "# **1. Define Initial Conditions and Parameters**\n",
    "# =======================\n",
    "\n",
    "# Initial Conditions\n",
    "Y0 = [100000, 0, 0, 200]  # [T0, I0, L0, V0]\n",
    "\n",
    "# Parameters\n",
    "d = 0.01\n",
    "beta = 0.0000002\n",
    "f = 0.001\n",
    "a = 0.2\n",
    "delta_I = 1 \n",
    "delta_L = 0.0039\n",
    "c = 23\n",
    "p = 4000\n",
    "lambda_param = 1000  # T0 * d\n",
    "Omega = 1\n",
    "\n",
    "t_initial = 0\n",
    "t_final = 400  # Expanded from 100 to 400 to cover ~1 year\n",
    "\n",
    "t = np.linspace(t_initial, t_final, 2000)  # Increased number of points for higher resolution\n",
    "\n",
    "# =======================\n",
    "# **2. Define ODE System**\n",
    "# =======================\n",
    "\n",
    "def system(Y, t):\n",
    "    T, I, L, V = Y\n",
    "    dT_dt = lambda_param - d * T - Omega * beta * V * T\n",
    "    dI_dt = (1 - f) * Omega * beta * V * T + a * L - delta_I * I\n",
    "    dL_dt = f * Omega * beta * V * T - a * L - delta_L * L\n",
    "    dV_dt = Omega * p * I - c * V\n",
    "    return [dT_dt, dI_dt, dL_dt, dV_dt]\n",
    "\n",
    "# =======================\n",
    "# **3. Solve ODE Numerically**\n",
    "# =======================\n",
    "\n",
    "solution = odeint(system, Y0, t)\n",
    "T, I, L, V = solution.T\n",
    "\n",
    "# =======================\n",
    "# **4. Compute and Print Maximum Values**\n",
    "# =======================\n",
    "\n",
    "T_max = np.max(T)\n",
    "I_max = np.max(I)\n",
    "L_max = np.max(L)\n",
    "V_max = np.max(V)\n",
    "\n",
    "print(f\"Maximum Values:\")\n",
    "print(f\"T_max: {T_max}\")\n",
    "print(f\"I_max: {I_max}\")\n",
    "print(f\"L_max: {L_max}\")\n",
    "print(f\"V_max: {V_max}\")\n",
    "\n",
    "# =======================\n",
    "# **5. Create Interpolators for Numerical Solutions**\n",
    "# =======================\n",
    "\n",
    "interp_T = interp1d(t, T, kind='cubic', fill_value='extrapolate')\n",
    "interp_I = interp1d(t, I, kind='cubic', fill_value='extrapolate')\n",
    "interp_L = interp1d(t, L, kind='cubic', fill_value='extrapolate')\n",
    "interp_V = interp1d(t, V, kind='cubic', fill_value='extrapolate')\n",
    "\n",
    "# =======================\n",
    "# **6. Define Function for Solutions**\n",
    "# =======================\n",
    "\n",
    "def func(t_input):\n",
    "    \"\"\"Function to provide the exact solution for training and testing.\"\"\"\n",
    "    T_val = interp_T(t_input)\n",
    "    I_val = interp_I(t_input)\n",
    "    L_val = interp_L(t_input)\n",
    "    V_val = interp_V(t_input)\n",
    "    return np.column_stack((T_val, I_val, L_val, V_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]: Define ODE Residual Without Scaling\n",
    "\n",
    "import torch\n",
    "import deepxde as dde\n",
    "\n",
    "# Define ODE residual\n",
    "def ode(t, Y):\n",
    "    T = Y[:, 0:1]\n",
    "    I = Y[:, 1:2]\n",
    "    L = Y[:, 2:3]\n",
    "    V = Y[:, 3:4]\n",
    "    \n",
    "    dT_dt = dde.grad.jacobian(Y, t, i=0)\n",
    "    dI_dt = dde.grad.jacobian(Y, t, i=1)\n",
    "    dL_dt = dde.grad.jacobian(Y, t, i=2)\n",
    "    dV_dt = dde.grad.jacobian(Y, t, i=3)\n",
    "    \n",
    "    return [\n",
    "        dT_dt - (lambda_param - d * T - Omega * beta * V * T),\n",
    "        dI_dt - ((1 - f) * Omega * beta * V * T + a * L - delta_I * I),\n",
    "        dL_dt - (f * Omega * beta * V * T - a * L - delta_L * L),\n",
    "        dV_dt - (Omega * p * I - c * V)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]: Redefine Neural Network and Model Without Scaling\n",
    "\n",
    "# Define input_transform and output_transform as identity functions\n",
    "def input_transform(t):\n",
    "    return t  # Identity transform (no change)\n",
    "\n",
    "def output_transform(t, y):\n",
    "    return y  # Identity transform (no change)\n",
    "\n",
    "# Define geometry with the expanded time domain\n",
    "geom = dde.geometry.TimeDomain(t_initial, t_final)\n",
    "\n",
    "# Define boundary condition (initial conditions)\n",
    "def boundary(t, on_initial):\n",
    "    return on_initial\n",
    "\n",
    "# Define initial conditions as hard constraints\n",
    "ic_T = dde.icbc.IC(geom, lambda t: Y0[0], boundary, component=0)\n",
    "ic_I = dde.icbc.IC(geom, lambda t: Y0[1], boundary, component=1)\n",
    "ic_L = dde.icbc.IC(geom, lambda t: Y0[2], boundary, component=2)\n",
    "ic_V = dde.icbc.IC(geom, lambda t: Y0[3], boundary, component=3)\n",
    "\n",
    "# Define data without scaling\n",
    "data = dde.data.PDE(\n",
    "    geom,\n",
    "    ode,\n",
    "    [ic_T, ic_I, ic_L, ic_V],\n",
    "    num_domain=5000,\n",
    "    num_boundary=4,  # 4 initial conditions\n",
    "    solution=func,    # Use the corrected func without scaling\n",
    "    num_test=1000\n",
    ")\n",
    "\n",
    "# Define the neural network with the same architecture\n",
    "neurons = 64\n",
    "layers = 4  # Number of hidden layers\n",
    "activation = \"tanh\"\n",
    "initializer = \"Glorot normal\"\n",
    "\n",
    "net = dde.nn.FNN([1] + [neurons] * layers + [4], activation, initializer)\n",
    "\n",
    "# Apply transforms\n",
    "net.apply_feature_transform(input_transform)\n",
    "net.apply_output_transform(output_transform)\n",
    "\n",
    "# Define the model with the new data and network\n",
    "model = dde.Model(data, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phase 1: Training with Adam optimizer...\n",
      "Training model...\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# In[15]: Train the Model with Two-Phase Strategy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Phase 1: Train with Adam optimizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Phase 1: Training with Adam optimizer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m losshistory, train_state \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced iterations for Phase 1\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Phase 2: Train with L-BFGS optimizer\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Phase 2: Training with L-BFGS optimizer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\utils\\internal.py:22\u001b[0m, in \u001b[0;36mtiming.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     21\u001b[0m     ts \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[1;32m---> 22\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     te \u001b[38;5;241m=\u001b[39m timeit\u001b[38;5;241m.\u001b[39mdefault_timer()\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:643\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, iterations, batch_size, display_every, disregard_previous_best, callbacks, model_restore_path, model_save_path, epochs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mset_data_train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtrain_next_batch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size))\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mset_data_test(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtest())\n\u001b[1;32m--> 643\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_train_begin()\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizers\u001b[38;5;241m.\u001b[39mis_external_optimizer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt_name):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:832\u001b[0m, in \u001b[0;36mModel._test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_test\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;66;03m# TODO Now only print the training loss in rank 0. The correct way is to print the average training loss of all ranks.\u001b[39;00m\n\u001b[0;32m    829\u001b[0m     (\n\u001b[0;32m    830\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39my_pred_train,\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mloss_train,\n\u001b[1;32m--> 832\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_outputs_losses\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_aux_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39my_pred_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mloss_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_losses(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mX_test,\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39my_test,\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39mtest_aux_vars,\n\u001b[0;32m    843\u001b[0m     )\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_state\u001b[38;5;241m.\u001b[39my_test, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\deepxde\\model.py:551\u001b[0m, in \u001b[0;36mModel._outputs_losses\u001b[1;34m(self, training, inputs, targets, auxiliary_vars)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mrequires_grad_(requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 551\u001b[0m     outs \u001b[38;5;241m=\u001b[39m \u001b[43moutputs_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliary_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mrequires_grad_()\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;66;03m# TODO: auxiliary_vars\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# In[15]: Train the Model with Two-Phase Strategy\n",
    "\n",
    "# Phase 1: Train with Adam optimizer\n",
    "print(\"Starting Phase 1: Training with Adam optimizer...\")\n",
    "losshistory, train_state = model.train(\n",
    "    iterations=10000,  # Reduced iterations for Phase 1\n",
    "    display_every=1000\n",
    ")\n",
    "\n",
    "# Phase 2: Train with L-BFGS optimizer\n",
    "print(\"\\nStarting Phase 2: Training with L-BFGS optimizer...\")\n",
    "model.compile(\"L-BFGS\")\n",
    "losshistory, train_state = model.train()\n",
    "\n",
    "# =======================\n",
    "# **6. Plotting Loss History**\n",
    "# =======================\n",
    "\n",
    "dde.utils.external.plot_loss_history(losshistory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
